{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Velocity', -0.012878723447980411),\n",
       " ('Position', 0.01592590876217463),\n",
       " ('Angle', -0.034427017752065796),\n",
       " ('Angular velocity', 0.01857454853460591)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: env\n",
    "except NameError: env = None\n",
    "if env: env.close()\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "list(zip(['Velocity', 'Position', 'Angle', 'Angular velocity'], obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardcoded policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "render = False\n",
    "policy = lambda obs: int(obs[2] > 0)  # Move to the same direction of the pole \n",
    "\n",
    "rewards = []\n",
    "for episode in range(100):\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(200):\n",
    "        action = policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            sleep(0.001)\n",
    "        if done:\n",
    "            rewards.append(episode_reward)\n",
    "            break    \n",
    "            \n",
    "print(f'Mean: {np.mean(rewards)}, Std: {np.std(rewards):.2f}, Min: {np.min(rewards)}, Max: {np.max(rewards)}'    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Policy Gradients `REINFORCE`\n",
    "\n",
    "1. First, let the neural network policy play the game several times, and at each step, compute the gradients that would make the chosen action even more likely but don’t apply these gradients yet.\n",
    "2. Once you have run several episodes, compute each action’s advantage (using the method described in the previous section).\n",
    "3. If an action’s advantage is positive, it means that the action was probably good, and you want to apply the gradients computed earlier to make the action even more likely to be chosen in the future. However, if the action’s advantage is negative, it means the action was probably bad, and you want to apply the opposite gradients to make this action slightly less likely in the future. The solution is simply to multiply each gradient vector by the corresponding action’s advantage.\n",
    "4. Finally, compute the mean of all the resulting gradient vectors, and use it to perform a Gradient Descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='elu', input_shape=[env.observation_space.shape[0]]),\n",
    "    Dense(1, activation='sigmoid'),  # Binary action, left or right\n",
    "])\n",
    "\n",
    "optimizer = Adam(lr=1e-2)\n",
    "loss_fn = binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down of the problem in helper function steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def run_one_step(env: gym.Env, obs: np.ndarray,\n",
    "                 model: tf.keras.Model, loss_fn: tf.keras.losses,\n",
    "                 render: bool):\n",
    "    \"\"\"Run one step of the environment. Option to render with 100 fps.\n",
    "    \n",
    "    - Compute and record gradients assuming all taken actions are correct\n",
    "    - Advance one step in the environment\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = tf.random.uniform([1, 1]) > left_proba\n",
    "        \n",
    "        # Assume always correct. i.e. left action == 0 -> left proba == 1\n",
    "        target_y = tf.constant([1.]) - tf.cast(action, tf.float32)\n",
    "        loss = loss_fn(target_y, left_proba)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action))\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "        sleep(0.01) # 100 fps\n",
    "    \n",
    "    return obs, reward, done, gradients\n",
    "\n",
    "\n",
    "def run_episodes(env: gym.Env, n_episodes: int, n_max_steps: int,\n",
    "                 model: tf.keras.Model, loss_fn: tf.keras.losses,\n",
    "                 render: bool = False):\n",
    "    \"\"\"Run multiple episodes looping over run_one_step\n",
    "    \n",
    "    - Record rewards and gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    all_gradients = []\n",
    "    all_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_gradients = []\n",
    "        episode_rewards = []\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, gradients = run_one_step(env, obs, model, loss_fn, render)\n",
    "            episode_rewards.append(reward)\n",
    "            episode_gradients.append(gradients)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        all_rewards.append(episode_rewards)       \n",
    "        all_gradients.append(episode_gradients)\n",
    "                \n",
    "    return all_gradients, all_rewards\n",
    "  \n",
    "\n",
    "def discount_all_rewards(all_rewards: '2D arary', epsilon):\n",
    "    \"\"\"Discount rewards (list of lists) with factor epsilon\"\"\"\n",
    "    \n",
    "    all_discounted_rewards = []    \n",
    "    for episode_rewards in all_rewards:\n",
    "        discounted_rewards = [episode_rewards[-1]]\n",
    "        for i in range(len(episode_rewards) - 1):\n",
    "            discounted_rewards.append(episode_rewards[-i + 1] + epsilon*discounted_rewards[i])\n",
    "        discounted_rewards.reverse()\n",
    "        \n",
    "        all_discounted_rewards.append(discounted_rewards)\n",
    "   \n",
    "    return all_discounted_rewards\n",
    "\n",
    "\n",
    "def normalize_all_rewards(all_rewards: '2D array'):\n",
    "    \"\"\"Normalize rewards (list of lists)\"\"\"   \n",
    "    \n",
    "    all_norm_rewards = []\n",
    "    for episode_rewards in all_rewards:\n",
    "        rewards_mean = np.mean(episode_rewards)\n",
    "        rewards_std = np.std(episode_rewards)\n",
    "        \n",
    "        normed_episode_rewards = (np.array(episode_rewards) - rewards_mean ) / rewards_std\n",
    "        \n",
    "        all_norm_rewards.append(normed_episode_rewards)\n",
    "    \n",
    "    return all_norm_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_stats(rewards):\n",
    "    rewards = [sum(reward) for reward in rewards]\n",
    "    rewards = np.array(rewards)\n",
    "    return f'Mean: {np.mean(rewards)}, Std: {np.std(rewards):.2f}, Min: {np.min(rewards)}, Max: {np.max(rewards)}'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 24.4, Std: 14.97, Min: 13.0, Max: 54.0\n",
      "Mean: 43.6, Std: 15.29, Min: 26.0, Max: 72.0\n",
      "Mean: 59.4, Std: 25.91, Min: 24.0, Max: 91.0\n",
      "Mean: 134.2, Std: 54.24, Min: 38.0, Max: 200.0\n",
      "Mean: 129.4, Std: 60.71, Min: 34.0, Max: 200.0\n",
      "Mean: 193.4, Std: 8.24, Min: 181.0, Max: 200.0\n",
      "Mean: 173.4, Std: 22.46, Min: 147.0, Max: 200.0\n",
      "Mean: 147.4, Std: 36.44, Min: 89.0, Max: 200.0\n",
      "Mean: 158.0, Std: 28.98, Min: 120.0, Max: 200.0\n",
      "Mean: 198.8, Std: 2.40, Min: 194.0, Max: 200.0\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 5\n",
    "n_max_steps = 200\n",
    "\n",
    "train_steps = 100\n",
    "epsilon=0.95\n",
    "\n",
    "for train_step in range(train_steps):\n",
    "    gradients, rewards = run_episodes(env=env, n_episodes=n_episodes, n_max_steps=n_max_steps, \n",
    "                                      model=model, loss_fn=loss_fn, render=False)\n",
    "    \n",
    "    if not train_step%10:\n",
    "        print(report_stats(rewards))\n",
    "    \n",
    "    discounted_rewards = discount_all_rewards(rewards, epsilon)\n",
    "    normalized_rewards = normalize_all_rewards(discounted_rewards)\n",
    "    \n",
    "    all_gradients = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        var_gradients = []\n",
    "        for episode_rewards, episode_gradients in zip(normalized_rewards, gradients):\n",
    "            for step_reward, step_gradients in zip(episode_rewards, episode_gradients):\n",
    "                var_gradients.append(step_reward*step_gradients[var_index])\n",
    "                \n",
    "        var_gradients = np.mean(var_gradients, axis=0)\n",
    "        all_gradients.append(var_gradients)\n",
    "                \n",
    "    optimizer.apply_gradients(zip(all_gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200\n"
     ]
    }
   ],
   "source": [
    "_, all_rewards = run_episodes(env, n_episodes=1, n_max_steps=n_max_steps,\n",
    "                              model=model, loss_fn=loss_fn, render=True)\n",
    "\n",
    "print(f'{int(sum(all_rewards[0]))}/{n_max_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
